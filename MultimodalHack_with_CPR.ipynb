{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultimodalHack with CPR - Tutorial\n",
    "\n",
    "This iPython Notebook was prepared by Daniele Di Mitri for the Learning Analytics Hackathon to take place in Amsterdam 23-24 October 2018. For more information about the Hackathon check here http://lsac2018.org/#hackathon\n",
    "\n",
    "\n",
    "## 1. The Multimodal Tutor for CPR \n",
    "The aim of this study is to test a new intelligent Multimodal Tutor for training people to perform cardiopulmonary resuscitation using patient manikins (CPR tutor). The tutor uses a multi-sensor setup for tracking the CPR execution and generating personalised feedback. This study is part of a PhD project focusing on multimodal data support for investigating practice-based learning scenarios, such as psychomotor skills training in the classroom or at the workplace. For the CPR tutor the multimodal data considered consist of trainee's body position recorded by  Microsoft Kinect camera, electromyogram recorded with Myo armband, and performance metrics derived from the manikin (Laerdal ResusciAnne QCPR manikin). This specific manikin can be linked with the Simpad SkillsReporter, a device capable of calculating reliable CPR performance metrics, such as *CompressionRate*, *CompressionDepth* and *CompressionRelease*. What the manikin is currently not capable of assessing is other important performance metrics of CPR such as arms being locked while doing the chest compressions, or whether the trainee uses his/her body weight to facilitate the chest compressions. \n",
    "![alt text](https://i.imgur.com/n7KYl4l.png \"The Multimodal Tutor for CPR\")\n",
    "\n",
    "### 1. Data collection: Multimodal Learning Hub\n",
    "The Multimodal Learning Hub (LearningHub) is a system that focuses on the data collection and data storing of multimodal learning experiences. It uses the concept of *Meaningful Learning Task* (MLT) and introduces a new data format (MLT session file) for data storing and exchange. The LearningHub implements a set of specifications that shape it for certain types of learning activities. It was created to be compatible primarily with commercial devices (e.g. Microsoft Kinect, Leap Motion, Myo Armband) and other sensors with drivers running with the most common operating systems. It focuses on short and meaningful learning activities (~10 minutes) and uses a distributed, client-server architecture with a master node controlling and receiving updates from multiple data-provider applications. It also handles video and audio recordings with the main purpose to support the human annotation process. The LearningHub is open source and developed in C# and can be found here https://github.com/janschneiderou/LearningHub\n",
    "\n",
    "### 2. Data storing: MLT sessions\n",
    "The expected output of the LearningHub is one (or multiple) MLT session files including 1) one-to-n multimodal, time-synchronised sensor recordings; 2) a video/audio file providing evidence for retrospective annotations. Each session is stored in a zip folder and has the following structure:\n",
    "\n",
    "**SessionFile.zip**\n",
    "+ annotations.json \n",
    "+ app1.json \n",
    "+ app2.json \n",
    "+ ... \n",
    "+ appn.json \n",
    "+ video.mp4 \n",
    "\n",
    "The **app1.json, app2.json, ... appN.json** are the files containing the sensor data (e.g. from Kinect and Myo armband). These files constitute the input data of the data analysis.\n",
    "\n",
    "The **annotation.json** contains the time-intervals annotated with with the *Visual Inspection Tool*. The labels contained in this file are used in the prediction/classissification file generated. \n",
    "\n",
    "### 3. Data annotation: Visual Inspection Tool  \n",
    "The data annotation is performed using the *Visual Inspection Tool* (VIT). The VIT allows to load the MLT session and segment the sensor data into time intervals and provide several key-value annotations. It also allows to load the annoations which are generated by the system manually. The VIT can be found open source on GitHub https://github.com/dimstudio/visual-inspection-tool \n",
    "\n",
    "\n",
    "## 2. Purpose of this Tutorial \n",
    "In this tutorial we focus on the step of the data-informed cylce. 4) The data Processing. We use some sample MLT sessions of the Multimodal Tutor. \n",
    "\n",
    "### Machine Learning Task description \n",
    "\n",
    "INPUT SPACE: all the non-zero attributes contained in the sensor data. Each of this attribute is a single time-series which has different time frequencies.  \n",
    "\n",
    "HYPOTHESIS SPACE: the space of hypothesis includes five binary (0 or 1) target attributes, three *CompressionRate*, *CompressionDepth* and *CompressionRelease*  derived by the CPR Manikin data, and two *BodyWeight* and *ArmsLocked* detected by the manikin. \n",
    "\n",
    "PROBLEM REPRESENTATION: we use Attribute-Value representation. Each time-interval (i.e. chest compression) is a row while each sensor attribute is a column. To aggregate each attribute into numerical features we use aggregating functions. A graphical representation of the problem can be found here \n",
    "![alt text](https://i.imgur.com/MJMaZrB.png \"Data trasformation visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Script\n",
    "### 1. Import some python libraries (NumPy, Pandas, SkLearn, etc. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set of imports\n",
    "import numpy as np\n",
    "\n",
    "import os \n",
    "import zipfile\n",
    "import json\n",
    "import re\n",
    "import operator\n",
    "import requests\n",
    "import StringIO\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize  \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Retrieve session file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sessions = []\n",
    "zip_file_urls = ['https://github.com/dimstudio/multimodal-analyzer/raw/master/example_sessions/OKDDM1_2018-7-24-11H57M49S371_annotated.zip']\n",
    "for zip_url in zip_file_urls:\n",
    "    r1 = requests.get(zip_url, stream=True)\n",
    "    z1 = StringIO.StringIO(r1.content)\n",
    "sessions.append(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data preparation in a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfALL = pd.DataFrame() # Dataframe with all summarised data\n",
    "dfAnn = pd.DataFrame() # Dataframe containing the annotations\n",
    "\n",
    "# for each session in the list of sessions\n",
    "for s in sessions:\n",
    "    \n",
    "    #1. Reading data from zip file\n",
    "    with zipfile.ZipFile(s) as z:\n",
    "        \n",
    "        # get current absolute time in seconds. This is necessary to add the delta correctly\n",
    "        for info in z.infolist():\n",
    "            file_datetime = datetime.datetime(*info.date_time)\n",
    "        current_time_offset = pd.to_datetime(pd.to_datetime(file_datetime, format='%H:%M:%S.%f'),unit='s')\n",
    "        \n",
    "         # First look for annotation.json\n",
    "        for filename in z.namelist():\n",
    "            \n",
    "            if not os.path.isdir(filename):\n",
    "                \n",
    "                if '.json' in filename:\n",
    "                    \n",
    "                    with z.open(filename) as f:\n",
    "                         data = json.load(f) \n",
    "                    # if it has the 'intervals' then then it is an annotatation file \n",
    "                    \n",
    "                    if 'intervals' in data:\n",
    "                        \n",
    "                        # concatenate the data with the intervals normalized and drop attribute 'intervals'\n",
    "                        df = pd.concat([pd.DataFrame(data), \n",
    "                            json_normalize(data['intervals'])], \n",
    "                            axis=1).drop('intervals', 1)\n",
    "                        \n",
    "                        # convert to numeric (when reading from JSON it converts into object in the pandas DF)\n",
    "                        # with the parameter 'ignore' it will skip all the non-numerical fields \n",
    "                        df = df.apply(pd.to_numeric, errors='ignore')\n",
    "                        \n",
    "                        # remove the prefix 'annotations.' from the column names\n",
    "                        df.columns = df.columns.str.replace(\"annotations.\", \"\")\n",
    "                        \n",
    "                        # from string to timedelta + offset\n",
    "                        df.start = pd.to_timedelta(df.start) + current_time_offset\n",
    "                        \n",
    "                        # from string to timedelta + offset\n",
    "                        df.end = pd.to_timedelta(df.end) + current_time_offset\n",
    "                        \n",
    "                        # duration as subtractions of delta in seconds\n",
    "                        df['duration'] = (df.end-df.start) / np.timedelta64(1, 's')   \n",
    "                        \n",
    "                        # append this dataframe to the dataframe annotations\n",
    "                        dfAnn = dfAnn.append(df) \n",
    "                        \n",
    "                    # if it has 'frames' then it is a sensor file \n",
    "                    elif 'frames' in data:\n",
    "                        \n",
    "                        # concatenate the data with the intervals normalized and drop attribute 'frames'\n",
    "                        df = pd.concat([pd.DataFrame(data), \n",
    "                            json_normalize(data['frames'])], \n",
    "                            axis=1).drop('frames', 1)\n",
    "                        \n",
    "                        # remove underscore from columnfile e.g. 3_Ankle_Left_X becomes 3AnkleLeftX\n",
    "                        df.columns = df.columns.str.replace(\"_\", \"\")\n",
    "                        \n",
    "                        # from string to timedelta + offset\n",
    "                        df['frameStamp']= pd.to_timedelta(df['frameStamp']) + current_time_offset\n",
    "                        \n",
    "                        # retrieve the applicaiton name\n",
    "                        appName = df.applicationName.all()\n",
    "                        \n",
    "                        # remove the prefix 'frameAttributes.' from the column names\n",
    "                        df.columns = df.columns.str.replace(\"frameAttributes\", df.applicationName.all())\n",
    "                        \n",
    "                        # set the timestamp as index \n",
    "                        df = df.set_index('frameStamp').iloc[:,2:]\n",
    "                        \n",
    "                        # exclude duplicates (taking the first occurence in case of duplicates)\n",
    "                        df = df[~df.index.duplicated(keep='first')]\n",
    "                        \n",
    "                        # convert to numeric (when reading from JSON it converts into object in the pandas DF)\n",
    "                        # with the parameter 'ignore' it will skip all the non-numerical fields \n",
    "                        df = df.apply(pd.to_numeric, errors='ignore')\n",
    "                        \n",
    "                        # Keep the numeric types only (categorical data are not supported now)\n",
    "                        df = df.select_dtypes(include=['float64','int64'])\n",
    "                        \n",
    "                        # Remove columns in which the sum of attributes is 0 (meaning there the information is 0)\n",
    "                        df = df.loc[:, (df.sum(axis=0) != 0)]\n",
    "                        \n",
    "                        # The application KienctReader can track up to 6 people, whose attributes are \n",
    "                        # 1ShoulderLeftX or 3AnkleRightY. We get rid of this numbers assuming there is only 1 user\n",
    "                        # This part has to be rethinked in case of 2 users\n",
    "                        df.rename(columns=lambda x: re.sub('Kinect.\\d','Kinect.',x),inplace=True)\n",
    "                        \n",
    "                        # Concate this dataframe in the dfALL and then sort dfALL by index\n",
    "                        dfALL = pd.concat([dfALL, df], ignore_index=False,sort=False).sort_index()\n",
    "                        \n",
    "                        \n",
    "df1 =  dfALL.apply(pd.to_numeric).fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature extraction with aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exclude irrelevant attributes \n",
    "to_exclude = ['Ankle']\n",
    "for el in to_exclude:\n",
    "    df1 = dfALL[[col for col in df1.columns if el not in col]]\n",
    "    \n",
    "# Feature aggregation functions \n",
    "aggregations = ['max','min','std','mean','var']\n",
    "\n",
    "# The masked_df allows to select intervals based on the annotaton \n",
    "masked_df = [\n",
    "    df1[(df2_start <= df1.index) & (df1.index <= df2_end)]\n",
    "    for df2_start, df2_end in zip(dfAnn['start'], dfAnn['end'])\n",
    "]\n",
    "\n",
    "features = []\n",
    "\n",
    "# For each attribute\n",
    "for key in df1.columns.values:\n",
    "    \n",
    "    # For each function function aggregation\n",
    "    for a in aggregations:\n",
    "        \n",
    "        # the name is attribute.aggregation \n",
    "        fname = key+'.'+a\n",
    "        \n",
    "        # append it as a feature\n",
    "        features.append(fname)\n",
    "        \n",
    "        # apply the aggregation function \n",
    "        if a == 'max':\n",
    "            dfAnn[fname] = [np.max(dt[key]) if not dt.empty else None for dt in masked_df]\n",
    "        elif a == 'min':\n",
    "            dfAnn[fname] = [np.min(dt[key]) if not dt.empty else None for dt in masked_df]\n",
    "        elif a == 'std':    \n",
    "            dfAnn[fname] = [np.std(dt[key]) if not dt.empty else None for dt in masked_df]\n",
    "        elif a == 'mean':    \n",
    "            dfAnn[fname] = [np.mean(dt[key]) if not dt.empty else None for dt in masked_df]\n",
    "        elif a == 'var':\n",
    "            dfAnn[fname] = [np.var(dt[key]) if not dt.empty else None for dt in masked_df]\n",
    "            \n",
    "dfAnn = dfAnn.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature importances with ETF and SVM classfier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-804674801cbf>, line 62)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-804674801cbf>\"\u001b[0;36m, line \u001b[0;32m62\u001b[0m\n\u001b[0;31m    print 'SVM '+target+' N='+str(np.shape(dfAnn)[0]) +' Accuracy = ' accuracy_score(y_test, y_pred)\u001b[0m\n\u001b[0m                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# The target features set\n",
    "target_features = ['classRate','classDepth','classRelease']\n",
    "\n",
    "\n",
    "# For each target feature in the feature set\n",
    "for target in target_features:\n",
    "    \n",
    "    # Scaling features  \n",
    "    #http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "    \n",
    "    # Instantiating the model\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    \n",
    "    # the domain is the valeus of the features \n",
    "    X = dfAnn[features].values\n",
    "\n",
    "    # Scaling features \n",
    "    X = min_max_scaler.fit_transform(X)\n",
    "    \n",
    "    # Range (target feature)\n",
    "    y = dfAnn[target].values.ravel()\n",
    "    \n",
    "    # ExtraTreeClassifier for feature importance\n",
    "    #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "    ETF = ExtraTreesClassifier()\n",
    "    ETF.fit(X, y)\n",
    "    \n",
    "    # Just sorting the dictionary by the feature with highest to the lowest importance\n",
    "    importance = sorted(dict(zip(dfAnn[features].columns, ETF.feature_importances_)).items(), \n",
    "                        key=operator.itemgetter(1),reverse=True)\n",
    "    \n",
    "    # Classification with SVM for classification linear kernel  \n",
    "    svc = SVC(kernel=\"linear\", C=1)\n",
    "    accuracies = []\n",
    "    \n",
    "    # Try from 1 to n_features to check the best number of features \n",
    "    #for n in range(1,len(importance)):\n",
    "        \n",
    "    training_feautres = []\n",
    "\n",
    "    # For each element in the feature set ordered for importance\n",
    "    for el in importance[:len(importance)]:\n",
    "        training_feautres.append(el[0])\n",
    "\n",
    "    # Get the value of these featues PLUS the target\n",
    "    dfAnn[training_feautres+[target]]\n",
    "\n",
    "    # The domain is composed by the Scaled training featues\n",
    "    X = min_max_scaler.fit_transform(dfAnn[training_feautres].values)\n",
    "\n",
    "    # The range is target values\n",
    "    y = dfAnn[target].values.ravel()\n",
    "\n",
    "    # Set the support vector model with test size at 33 % and a random seed\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=72)\n",
    "    svc.fit(X_train, y_train)\n",
    "\n",
    "    # Prediction\n",
    "    y_pred = svc.predict(X_test)\n",
    "\n",
    "    # Append to the accuracy list\n",
    "    print 'SVM '+target+' N='+str(np.shape(dfAnn)[0]) +' Accuracy = ' accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  \\*\\*\\*\\* End tutorial \\*\\*\\*\\*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for the Hackathon:\n",
    "+ How can we overall improved the prediction accuracy of the classification models?\n",
    "+ Can we find better features (e.g. FFT etc)? For example tools such as https://tech.blue-yonder.com/tsfresh-scientific-and-free/\n",
    "+ Train the same classifier on other sessions individually?\n",
    "+ Train the classifier on other sessions together?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
