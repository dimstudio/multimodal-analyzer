{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultimodalHack with CPR - Tutorial\n",
    "\n",
    "This iPython Notebook was prepared by Daniele Di Mitri for the Learning Analytics Hackathon to take place in Amsterdam 23-24 October 2018. For more information about the Hackathon check here http://lsac2018.org/#hackathon\n",
    "\n",
    "\n",
    "## 1. The Multimodal Tutor for CPR \n",
    "The aim of this study is to test a new intelligent Multimodal Tutor for training people to perform cardiopulmonary resuscitation using patient manikins (CPR tutor). The tutor uses a multi-sensor setup for tracking the CPR execution and generating personalised feedback. This study is part of a PhD project focusing on multimodal data support for investigating practice-based learning scenarios, such as psychomotor skills training in the classroom or at the workplace. For the CPR tutor the multimodal data considered consist of trainee's body position recorded by  Microsoft Kinect camera, electromyogram recorded with Myo armband, and performance metrics derived from the manikin (Laerdal ResusciAnne QCPR manikin). This specific manikin can be linked with the Simpad SkillsReporter, a device capable of calculating reliable CPR performance metrics, such as *CompressionRate*, *CompressionDepth* and *CompressionRelease*. What the manikin is currently not capable of assessing is other important performance metrics of CPR such as arms being locked while doing the chest compressions, or whether the trainee uses his/her body weight to facilitate the chest compressions. \n",
    "![alt text](https://i.imgur.com/n7KYl4l.png \"The Multimodal Tutor for CPR\")\n",
    "\n",
    "### 1. Data collection: Multimodal Learning Hub\n",
    "The Multimodal Learning Hub (LearningHub) is a system that focuses on the data collection and data storing of multimodal learning experiences. It uses the concept of *Meaningful Learning Task* (MLT) and introduces a new data format (MLT session file) for data storing and exchange. The LearningHub implements a set of specifications that shape it for certain types of learning activities. It was created to be compatible primarily with commercial devices (e.g. Microsoft Kinect, Leap Motion, Myo Armband) and other sensors with drivers running with the most common operating systems. It focuses on short and meaningful learning activities (~10 minutes) and uses a distributed, client-server architecture with a master node controlling and receiving updates from multiple data-provider applications. It also handles video and audio recordings with the main purpose to support the human annotation process. The LearningHub is open source and developed in C# and can be found here https://github.com/janschneiderou/LearningHub\n",
    "\n",
    "### 2. Data storing: MLT sessions\n",
    "The expected output of the LearningHub is one (or multiple) MLT session files including 1) one-to-n multimodal, time-synchronised sensor recordings; 2) a video/audio file providing evidence for retrospective annotations. Each session is stored in a zip folder and has the following structure:\n",
    "\n",
    "**SessionFile.zip**\n",
    "+ annotations.json \n",
    "+ app1.json \n",
    "+ app2.json \n",
    "+ ... \n",
    "+ appn.json \n",
    "+ video.mp4 \n",
    "\n",
    "The **app1.json, app2.json, ... appN.json** are the files containing the sensor data (e.g. from Kinect and Myo armband). These files constitute the input data of the data analysis.\n",
    "\n",
    "The **annotation.json** contains the time-intervals annotated with with the *Visual Inspection Tool*. The labels contained in this file are used in the prediction/classissification file generated. \n",
    "\n",
    "### 3. Data annotation: Visual Inspection Tool  \n",
    "The data annotation is performed using the *Visual Inspection Tool* (VIT), a tool which allows to load the MLT session and segment the sensor data into time intervals and provide annotations. The VIT can be found here https://github.com/dimstudio/visual-inspection-tool \n",
    "\n",
    "\n",
    "## 2. Purpose of this Tutorial \n",
    "In this tutorial we focus on the step 4. The data Processing. We use some sample MLT sessions of the Multimodal Tutor. \n",
    "\n",
    "### Machine Learning Task description \n",
    "\n",
    "INPUT SPACE: all the non-zero attributes contained in the sensor data. Each of this attribute is a single time-series which has different time frequencies.  \n",
    "\n",
    "HYPOTHESIS SPACE: the space of hypothesis includes five binary (0 or 1) target attributes, three *CompressionRate*, *CompressionDepth* and *CompressionRelease*  derived by the CPR Manikin data, and two *BodyWeight* and *ArmsLocked* detected by the manikin. \n",
    "\n",
    "HYPOTHESIS SPACE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Script\n",
    "### 1. Import some python libraries (NumPy, Pandas, SkLearn, etc. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set of imports\n",
    "import numpy as np\n",
    "\n",
    "import os \n",
    "import zipfile\n",
    "import json\n",
    "import re #regular expressions\n",
    "import operator\n",
    "import requests\n",
    "import StringIO\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize  \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Retrieve session file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sessions = []\n",
    "zip_file_urls = ['https://github.com/dimstudio/multimodal-analyzer/raw/master/example_sessions/OKDDM1_2018-7-24-11H57M49S371_annotated.zip']\n",
    "for zip_url in zip_file_urls:\n",
    "    r = requests.get(zip_url, stream=True)\n",
    "    z = StringIO.StringIO(r.content)\n",
    "sessions.append(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data preparation in a pandas DataFrame\n",
    "The following steps are represented in pseudo-code in the following figure\n",
    "![alt text](https://i.imgur.com/MJMaZrB.png \"Data trasformation visualized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfALL = pd.DataFrame() # Dataframe with all summarised data\n",
    "dfAnn = pd.DataFrame() # Dataframe containing the annotations\n",
    "\n",
    "# for each session in the list of sessions\n",
    "for s in sessions:\n",
    "    \n",
    "    #1. Reading data from zip file\n",
    "    with zipfile.ZipFile(s) as z:\n",
    "        \n",
    "        # get current absolute time in seconds. This is necessary to add the delta correctly\n",
    "        for info in z.infolist():\n",
    "            file_datetime = datetime(*info.date_time)\n",
    "        current_time_offset = pd.to_datetime(pd.to_datetime(file_datetime, format='%H:%M:%S.%f'),unit='s')\n",
    "        \n",
    "         # First look for annotation.json\n",
    "        for filename in z.namelist():\n",
    "            \n",
    "            if not os.path.isdir(filename):\n",
    "                \n",
    "                if '.json' in filename:\n",
    "                    \n",
    "                    with z.open(filename) as f:\n",
    "                         data = json.load(f) \n",
    "                    # if it has the 'intervals' then then it is an annotatation file \n",
    "                    \n",
    "                    if 'intervals' in data:\n",
    "                        \n",
    "                        # concatenate the data with the intervals normalized and drop attribute 'intervals'\n",
    "                        df = pd.concat([pd.DataFrame(data), \n",
    "                            json_normalize(data['intervals'])], \n",
    "                            axis=1).drop('intervals', 1)\n",
    "                        \n",
    "                        # convert to numeric (when reading from JSON it converts into object in the pandas DF)\n",
    "                        # with the parameter 'ignore' it will skip all the non-numerical fields \n",
    "                        df = df.apply(pd.to_numeric, errors='ignore')\n",
    "                        \n",
    "                        # remove the prefix 'annotations.' from the column names\n",
    "                        df.columns = df.columns.str.replace(\"annotations.\", \"\")\n",
    "                        \n",
    "                        # from string to timedelta + offset\n",
    "                        df.start = pd.to_timedelta(df.start) + current_time_offset\n",
    "                        \n",
    "                        # from string to timedelta + offset\n",
    "                        df.end = pd.to_timedelta(df.end) + current_time_offset\n",
    "                        \n",
    "                        # duration as subtractions of delta in seconds\n",
    "                        df['duration'] = (df.end-df.start) / np.timedelta64(1, 's')   \n",
    "                        \n",
    "                        # append this dataframe to the dataframe annotations\n",
    "                        dfAnn = dfAnn.append(df) \n",
    "                        \n",
    "                    # if it has 'frames' then it is a sensor file \n",
    "                    elif 'frames' in data:\n",
    "                        \n",
    "                        # concatenate the data with the intervals normalized and drop attribute 'frames'\n",
    "                        df = pd.concat([pd.DataFrame(data), \n",
    "                            json_normalize(data['frames'])], \n",
    "                            axis=1).drop('frames', 1)\n",
    "                        \n",
    "                        # remove underscore from columnfile e.g. 3_Ankle_Left_X becomes 3AnkleLeftX\n",
    "                        df.columns = df.columns.str.replace(\"_\", \"\")\n",
    "                        \n",
    "                        # from string to timedelta + offset\n",
    "                        df['frameStamp']= pd.to_timedelta(df['frameStamp']) + current_time_offset\n",
    "                        \n",
    "                        # retrieve the applicaiton name\n",
    "                        appName = df.applicationName.all()\n",
    "                        \n",
    "                        # remove the prefix 'frameAttributes.' from the column names\n",
    "                        df.columns = df.columns.str.replace(\"frameAttributes\", df.applicationName.all())\n",
    "                        \n",
    "                        # set the timestamp as index \n",
    "                        df = df.set_index('frameStamp').iloc[:,2:]\n",
    "                        \n",
    "                        # exclude duplicates (taking the first occurence in case of duplicates)\n",
    "                        df = df[~df.index.duplicated(keep='first')]\n",
    "                        \n",
    "                        # convert to numeric (when reading from JSON it converts into object in the pandas DF)\n",
    "                        # with the parameter 'ignore' it will skip all the non-numerical fields \n",
    "                        df = df.apply(pd.to_numeric, errors='ignore')\n",
    "                        \n",
    "                        # Keep the numeric types only (categorical data are not supported now)\n",
    "                        df = df.select_dtypes(include=['float64','int64'])\n",
    "                        \n",
    "                        # Remove columns in which the sum of attributes is 0 (meaning there the information is 0)\n",
    "                        df = df.loc[:, (df.sum(axis=0) != 0)]\n",
    "                        \n",
    "                        # The application KienctReader can track up to 6 people, whose attributes are \n",
    "                        # 1ShoulderLeftX or 3AnkleRightY. We get rid of this numbers assuming there is only 1 user\n",
    "                        # This part has to be rethinked in case of 2 users\n",
    "                        df.rename(columns=lambda x: re.sub('Kinect.\\d','Kinect.',x),inplace=True)\n",
    "                        \n",
    "                        # Concate this dataframe in the dfALL and then sort dfALL by index\n",
    "                        dfALL = pd.concat([dfALL, df], ignore_index=False,sort=False).sort_index()\n",
    "                        \n",
    "                        \n",
    "df1 =  dfALL.apply(pd.to_numeric).fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature extraction with aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exclude features\n",
    "to_exclude = ['Ankle']\n",
    "for el in to_exclude:\n",
    "    df1 = dfALL[[col for col in df1.columns if el not in col]]\n",
    "    \n",
    "# Feature aggregators\n",
    "aggregations = ['max','min','std','mean','var','median']\n",
    "\n",
    "# The masked_df allows to select intervals based on the annotaton \n",
    "masked_df = [\n",
    "    df1[(df2_start <= df1.index) & (df1.index <= df2_end)]\n",
    "    for df2_start, df2_end in zip(dfAnn['start'], dfAnn['end'])\n",
    "]\n",
    "\n",
    "features = []\n",
    "\n",
    "# For each attribute\n",
    "for key in df1.columns.values:\n",
    "    # For each function function aggregation\n",
    "    for a in aggregations:\n",
    "        # the name is attribute.aggregation \n",
    "        fname = key+'.'+a\n",
    "        # append it as a feature\n",
    "        features.append(fname)\n",
    "        # apply the aggregation function \n",
    "        if a == 'max':\n",
    "            dfAnn[fname] = [np.max(dt[key]) if not dt.empty else None for dt in masked_df]\n",
    "        elif a == 'min':\n",
    "            dfAnn[fname] = [np.min(dt[key]) if not dt.empty else None for dt in masked_df]\n",
    "        elif a == 'std':    \n",
    "            dfAnn[fname] = [np.std(dt[key]) if not dt.empty else None for dt in masked_df]\n",
    "        elif a == 'mean':    \n",
    "            dfAnn[fname] = [np.mean(dt[key]) if not dt.empty else None for dt in masked_df]\n",
    "        elif a == 'var':\n",
    "            dfAnn[fname] = [np.var(dt[key]) if not dt.empty else None for dt in masked_df]\n",
    "        elif a == 'median':  \n",
    "            dfAnn[fname] = [np.median(dt[key]) if not dt.empty else None for dt in masked_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature importances with ETF and SVM classfier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.19523600e-01  -1.23214500e-01   9.06644542e-04 ...,  -3.05320750e-01\n",
      "    8.04840211e-04              nan]\n",
      " [ -9.57875600e-02  -1.20397100e-01   7.84879930e-03 ...,  -2.91025817e-01\n",
      "    5.27951507e-05              nan]\n",
      " [ -1.02635200e-01  -1.13517200e-01   3.45156233e-03 ...,  -3.13644425e-01\n",
      "    4.90514963e-05              nan]\n",
      " ..., \n",
      " [ -6.84326400e-02  -9.77150500e-02   6.65073074e-03 ...,  -4.72656300e-01\n",
      "    0.00000000e+00              nan]\n",
      " [ -6.38721300e-02  -8.34100500e-02   6.58988692e-03 ...,  -4.94934100e-01\n",
      "    3.08148791e-33              nan]\n",
      " [ -6.46612900e-02  -8.98260000e-02   7.98977854e-03 ...,  -4.97497600e-01\n",
      "    3.35256100e-08              nan]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-126af1d3b2ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Scaling features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_max_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Range (target feature)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/daniele/anaconda/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/daniele/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/daniele/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         X = check_array(X, copy=self.copy, warn_on_dtype=True,\n\u001b[0;32m--> 334\u001b[0;31m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/daniele/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/daniele/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# The target features set\n",
    "target_features = ['classRate','classDepth','classRelease']\n",
    "# For each target feature in the feature set\n",
    "for target in target_features:\n",
    "    \n",
    "    # Scaling features  http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "    \n",
    "    # Instantiating the model\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    \n",
    "    # the domain is the valeus of the features \n",
    "    X = dfAnn[features].values\n",
    "    print X\n",
    "\n",
    "    # Scaling features \n",
    "    X = min_max_scaler.fit_transform(X)\n",
    "    \n",
    "    # Range (target feature)\n",
    "    y = dfAnn[target].values.ravel()\n",
    "    \n",
    "    # ExtraTreeClassifier for feature importance\n",
    "    #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "    ETF = ExtraTreesClassifier()\n",
    "    ETF.fit(X, y)\n",
    "    # Just sorting the dictionary by the feature with highest to the lowest importance\n",
    "    importance = sorted(dict(zip(dfAnn[features].columns, ETF.feature_importances_)).items(), \n",
    "                        key=operator.itemgetter(1),reverse=True)\n",
    "    # Classification with SVM for classification linear kernel  \n",
    "    svc = SVC(kernel=\"linear\", C=1)\n",
    "    accuracies = []\n",
    "    \n",
    "    # Try from 1 to n_features to check the best number of features \n",
    "    for n in range(1,len(importance)):\n",
    "        \n",
    "        training_feautres = []\n",
    "        \n",
    "        # For each element in the feature set ordered for importance\n",
    "        for el in importance[:n]:\n",
    "            training_feautres.append(el[0])\n",
    "            \n",
    "        # Get the value of these featues PLUS the target\n",
    "        dfAnn[training_feautres+[target]]\n",
    "        \n",
    "        # The domain is composed by the Scaled training featues\n",
    "        X = min_max_scaler.fit_transform(dfAnn[training_feautres].values)\n",
    "        \n",
    "        # The range is target values\n",
    "        y = dfAnn[target].values.ravel()\n",
    "        \n",
    "        # Set the support vector model with test size at 33 % and a random seed\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=72)\n",
    "        svc.fit(X_train, y_train)\n",
    "        # Prediction\n",
    "        y_pred = svc.predict(X_test)\n",
    "        # Append to the accuracy list\n",
    "        \n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "            \n",
    "    # Plot the figure of the accuracy\n",
    "    fig = plt.figure()\n",
    "    plt.plot(accuracies)\n",
    "    fig.suptitle('SVM '+target+' N='+str(np.shape(dfAnn)[0]), fontsize=20)\n",
    "    plt.xlabel('No. features', fontsize=18)\n",
    "    plt.ylabel('Accuracy', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
